# Note: See the slideshow PDF in the root directory. There is also the text examples given in the root directory in a PDF.


# AI Text Detector

A machine learning application that distinguishes between human-written and AI-generated text using a fine-tuned DistilBERT model.

## Overview

This project uses natural language processing to analyze text and determine whether it was likely written by a human or generated by AI. The system provides confidence scores and text statistics through a clean, modern web interface.

## Features

- **AI Detection**: Analyzes text for patterns characteristic of AI generation
- **Confidence Rating**: Provides detailed confidence categories rather than simple binary classification
- **Visualization**: Shows how the model tokenizes and processes text
- **Text Statistics**: Provides word count and character count metrics
- **Minimum Text Requirements**: Ensures accurate analysis by requiring at least 100 words

## Technology Stack

### Backend
- Python/Flask API server
- HuggingFace Transformers for DistilBERT model
- PyTorch for deep learning
- Sliding window approach for handling long texts

### Frontend
- Next.js React framework
- Tailwind CSS for styling
- shadcn/ui component library
- Server-side API proxying

## How It Works

1. User pastes text (minimum 100 words) into the interface
2. Text is sent to the Flask backend through Next.js API routes
3. The DistilBERT model analyzes the text using:
   - Tokenization into subwords
   - Processing through transformer layers
   - Sliding window technique for long texts
4. Results are returned showing:
   - Confidence category (Definitely Human, Likely Human, Uncertain, Likely AI, Definitely AI)
   - Confidence percentages
   - Text statistics

## Project Structure

```
ai-text-detector/
├── backend/                   # Flask API server
│   ├── app.py                # Main server file with model integration
│   ├── improved_ai_detector_model/  # Fine-tuned model files
│   │   ├── config.json
│   │   ├── model.safetensors
│   │   ├── special_tokens_map.json
│   │   ├── tokenizer_config.json
│   │   └── vocab.txt
│   └── requirements.txt      # Python dependencies
└── frontend/                 # Next.js frontend
    ├── app/                  # Next.js app router components
    ├── components/           # UI components
    │   ├── ModelExplanation.tsx
    │   ├── TokenizationVisualization.tsx
    │   └── ui/               # shadcn UI components
    └── [other configuration files]
```

## Setup Instructions

### Backend
```bash
cd backend
pip install -r requirements.txt
python app.py
```

### Frontend
```bash
cd frontend
npm install
npm run dev
```

The application will be available at:
- Frontend: http://localhost:3000
- Backend API: http://localhost:5000

## Model Details

- Based on DistilBERT, a lighter and faster version of BERT
- Fine-tuned on the NabeelShar/ai_and_human_text dataset
- Uses 512 token windows with overlap for analyzing long texts
- Temperature scaling for improved probability calibration

## Acknowledgements

- HuggingFace for the Transformers library
- The creators of DistilBERT
- The NabeelShar/ai_and_human_text dataset creators
